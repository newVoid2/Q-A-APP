{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab16011",
   "metadata": {},
   "source": [
    "# LangChain Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dec4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb707e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\r\n",
      "Version: 0.0.335\r\n",
      "Summary: Building applications with LLMs through composability\r\n",
      "Home-page: https://github.com/langchain-ai/langchain\r\n",
      "Author: \r\n",
      "Author-email: \r\n",
      "License: MIT\r\n",
      "Location: /Users/richard/opt/anaconda3/lib/python3.9/site-packages\r\n",
      "Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb24f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9641f",
   "metadata": {},
   "source": [
    "### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc8af0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcp-starter'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "os.environ.get('OPENAI_API_KEY')\n",
    "os.environ.get('PINECONE_API_KEY')\n",
    "os.environ.get('PINECONE_ENV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29927d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 512}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=512)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3275ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Computer engineering is the design, development, and maintenance of computer hardware, software, and networks.\n"
     ]
    }
   ],
   "source": [
    "output = llm('explain computer engineering in one sentence')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee10e1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_num_tokens('explain computer engineering in one sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa8ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.generate(['... is the capital of England', 'What is the largest animal in the world?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e73889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='?\\n\\nLondon.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe largest animal in the world is the Blue Whale. It can reach lengths of up to 100 feet and weigh up to 200 tons.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
     ]
    }
   ],
   "source": [
    "print(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521b8f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The largest animal in the world is the Blue Whale. It can reach lengths of up to 100 feet and weigh up to 200 tons.\n"
     ]
    }
   ],
   "source": [
    "print(output.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed170c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "779be0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.generate(['Write an original tagline for sushi restaurant'] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44adc3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\n\"Sushi So Good It\\'s Almost Too Good To Be True!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Savor the flavor of fresh sushi!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Satisfying sushi and more - Experience the Taste of Japan!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 43, 'total_tokens': 67, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('af8a4425-b9f4-47eb-90ae-86116bfc1544')), RunInfo(run_id=UUID('935817fd-6aeb-43d7-8b77-cbe888195ba2')), RunInfo(run_id=UUID('e36f1e34-e478-47c3-8c92-90daff6d675b'))]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8edf8351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Sushi So Good It's Almost Too Good To Be True!\"\n",
      "\n",
      "\"Savor the flavor of fresh sushi!\"\n",
      "\n",
      "\"Satisfying sushi and more - Experience the Taste of Japan!\""
     ]
    }
   ],
   "source": [
    "for output in output.generations:\n",
    "    print(output[0].text, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d461cc",
   "metadata": {},
   "source": [
    "### ChatModels: GPT-3.5-Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6201f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(AIMessage, HumanMessage, SystemMessage)\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "112247b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name='gpt-4', temperature=0.5, max_tokens=1024)\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist.'),\n",
    "    HumanMessage(content='explain quantum mechanics in one sentence') \n",
    "]\n",
    "output = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae2bfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is a branch of physics that deals with phenomena on a very small scale, such as molecules, atoms, and subatomic particles, where particles can exist in multiple states at once and change state due to observation, described through probabilities rather than definite outcomes.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81b169",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9628992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7259024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language', 'virus'] template='You are an experienced virologist.\\nWrite a few sentences about the following {virus} in {language}.'\n"
     ]
    }
   ],
   "source": [
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following {virus} in {language}.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['virus', 'language'],\n",
    "    template=template\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04163269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "एचआईवी एक ऐसा वायरस है जो हामी इंसुलिन सिस्टम (एचआईवी आईडी) को हैंडल करता है। यह अपने आप में बहुत तेजी से फैलता है और व्यक्ति को एचआईवी से जुड़े रोगों को संभावित कर\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7)\n",
    "output = llm(prompt.format(virus='HIV', language='hindi'))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bd75d",
   "metadata": {},
   "source": [
    "### Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9764c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4', temperature=0.5)\n",
    "\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following {virus} in {language}.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['virus', 'language'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "output = chain.run({'virus': 'HSV', 'language': 'french'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38933585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'HSV, ou virus de l'herpès simplex, est un virus qui provoque des infections courantes chez l'homme. Il existe deux types de ce virus : HSV-1, qui cause principalement l'herpès oral, et HSV-2, qui est principalement responsable de l'herpès génital. Les symptômes peuvent inclure des douleurs, des démangeaisons et des plaies dans les zones touchées.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a474c3",
   "metadata": {},
   "source": [
    "### Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59af7add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "def decision_tree(data, outcome):\n",
      "    \"\"\"\n",
      "    This function uses a decision tree approach to classify a given dataset based on a given outcome.\n",
      "    :param data: The dataset to be classified\n",
      "    :param outcome: The outcome to be used for classification\n",
      "    :return: A dictionary of the decision tree\n",
      "    \"\"\"\n",
      "    # Find out the unique values in the outcome column\n",
      "    unique_values = set(data[outcome])\n",
      "    \n",
      "    # Initialize a dictionary to store the decision tree\n",
      "    decision_tree = {}\n",
      "    \n",
      "    # Iterate through the unique outcome values\n",
      "    for value in unique_values:\n",
      "        # Create a sub-dataset for each outcome value\n",
      "        sub_data = data[data[outcome] == value]\n",
      "        # Calculate the entropy of the sub-dataset\n",
      "        entropy_sub_data = entropy(sub_data, outcome)\n",
      "        # Find the best feature to split the dataset on\n",
      "        best_feature = find_best_feature(sub_data, outcome)\n",
      "        # Split the dataset on the best feature\n",
      "        sub_data_split = split_data(sub_data, best_feature)\n",
      "        # Create a node in the decision tree for the best feature\n",
      "        decision_tree[best_feature] = {}\n",
      "        # Iterate through the sub-datasets created\n",
      "        for split in sub_data_split:\n",
      "            # Recursively call the decision_tree function on the sub-dataset\n",
      "            decision_tree[best_feature][split] = decision_tree(sub_data_split[split], outcome)\n",
      "            \n",
      "    return decision_tree\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe function `decision_tree` is a recursive function that constructs a decision tree classifier based on the input dataset provided and the outcome of interest. It accepts two arguments: `data` and `outcome`. `data` is the dataset to be classified and `outcome` is the variable in the dataset to be predicted.\n",
      "\n",
      "Firstly, the function finds the unique values in the outcome column of the dataset. This is done to determine the different possible classes in the outcome variable.\n",
      "\n",
      "Then, an empty dictionary named `decision_tree` is initialized to store the structure of the decision tree being built.\n",
      "\n",
      "The function then iterates through the unique values in the outcome column. For each unique value, a sub-dataset is created that only contains rows where the outcome is that value. The entropy of the sub-dataset is then calculated. Entropy is a measure of uncertainty or randomness. In the context of decision trees, it is used to measure the impurity of an input set.\n",
      "\n",
      "Next, the function determines the best feature in the dataset to split on. This is usually the feature that results in the largest information gain when the dataset is split on it. The dataset is then split based on this feature.\n",
      "\n",
      "A node in the decision tree is then created for this feature, and the dataset is split into subsets based on the values of this feature. These subsets are stored as key-value pairs in the decision tree dictionary, with the feature value being the key and the subset being the value.\n",
      "\n",
      "The function then recursively calls itself on each of these subsets. This is the process of building the decision tree, where each subset of the data is further split on the best feature until all data is classified correctly or a stopping condition is met.\n",
      "\n",
      "Finally, the function returns the decision tree dictionary, which represents the decision tree classifier.\n",
      "\n",
      "This function does not actually implement the entropy and find_best_feature functions, so it is assumed those are implemented elsewhere in the code. The split_data function is also not implemented in this snippet, and it is assumed to split the data based on the given feature.\n",
      "\n",
      "This function is a basic implementation of a decision tree algorithm, which is a popular machine learning method used for both classification and regression tasks. Decision trees work by creating a tree-like model of decisions based on the features in the data.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=1024)\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=['concept'],\n",
    "    template='''You are an experienced scientist and and Python programmer.\n",
    "    Write a function that implements the concept of {concept}.'''\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-4', temperature=0.7, max_tokens=1024)\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=['function'],\n",
    "    template='''Given the Python function {function}, describe it as detailed as possible.'''\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "output = overall_chain.run('decision tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd14a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
